{"pages":["Mastering PySpark and Spark Sql: 100+ Questions with Real-World Scenarios, Syntax & Explanations ✨ Author’s Note By Suden Gorai Dear Readers, It gives me immense pleasure to present this comprehensive resource — “Master PySpark & Spark SQL – At a Glance” — designed to empower data professionals, job-seekers, and engineering teams with deep yet digestible knowledge of Spark’s most powerful components: PySpark and Spark SQL. This guide is the result of hands-on experience solving real-world data problems, preparing for technical interviews, and optimizing production-grade data pipelines. I’ve organized it to be both a learning companion and an interview reference, offering clean formatting, syntax-highlighted examples, and scenario-driven insights. \uD83D\uDCD8 What’s Inside This Guide This eBook is structured into three major parts: \uD83E\uDDE9 Part 1: PySpark Questions (1–50) Covers core APIs, transformations, actions, performance tuning, and DataFrame operations with syntax and practical examples. \uD83E\uDDE9 Part 2: Spark SQL Questions (1–50) Explores query execution, joins, window functions, Delta Lake, schema evolution, and streaming use cases — all with real-world SQL syntax. \uD83D\uDD25 Bonus Section: Real-World Scenario-Based Questions A collection of intermediate to complex Spark SQL and PySpark scenarios, inspired by actual business challenges and use cases you’ll likely face in data engineering roles. \uD83D\uDEE0️ Add-On: Real-World Case Studies Included This guide goes beyond theory — I've included 3 practical projects, each with architecture diagrams, annotated code snippets, and clear explanations to demonstrate how PySpark and Spark SQL are applied at scale: \uD83D\uDD01 Project 1: SCD Type 2 Implementation using Delta Lake + PySpark Track historical changes in dimension tables while ensuring performance and audit compliance. \uD83D\uDCE1 Project 2: Real-Time Kafka Ingestion with Spark SQL Streaming + Dashboard Layer Ingest and query live data with streaming SQL logic, perfect for fraud detection or event monitoring. Sud en G ora i","⚡ Project 3: ETL Optimization with Fact-Dim Join using Partition Pruning & Broadcast Hints Maximize efficiency in star-schema joins by minimizing shuffles and skew. \uD83D\uDD39 Starting with PySpark – Section A: Basics & Architecture (Questions 1–10) 1. What is PySpark and how does it relate to Apache Spark? Answer: PySpark is the Python API for Apache Spark, an open-source distributed computing system. It allows Python developers to leverage Spark's capabilities like parallel computation, in-memory processing, and data pipeline creation using Python code. \uD83D\uDCA1 Use case: Python developers can perform ETL, analytics, and ML on large datasets. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"example\").getOrCreate() 2. How does the Spark execution engine differ from Hadoop MapReduce? Answer: • Spark processes data in-memory, reducing I/O overhead. • It supports lazy evaluation and DAG execution plans. • Much faster than Hadoop MapReduce, especially for iterative operations (ML, graph). 3. What are RDDs in PySpark? Answer: RDDs (Resilient Distributed Datasets) are the core data structure in Spark, representing an immutable distributed collection of objects processed in parallel. rdd = spark.sparkContext.parallelize([1, 2, 3, 4]) They support transformations (map, filter) and actions (collect, count). 4. What is the SparkContext? Answer: SparkContext is the entry point for using low-level Spark features like RDDs. It's automatically created with SparkSession in PySpark 2.x+. sc = spark.sparkContext Sud en G ora i","5. Explain the SparkSession in PySpark. Answer: SparkSession is the entry point to PySpark from Spark 2.x onwards. It allows access to DataFrames, SQL, and underlying contexts like SparkContext. spark = SparkSession.builder \\ .appName(\"MyApp\") \\ .getOrCreate() 6. How does lazy evaluation work in PySpark? Answer: Transformations like map, filter, etc., are not executed immediately. Instead, Spark builds a DAG and only runs the operations when an action (e.g., collect()) is triggered. This helps in optimizing the execution plan. 7. What is the DAG in Spark? Answer: DAG = Directed Acyclic Graph. Spark builds a logical execution plan (DAG) representing the sequence of transformations to be applied on data. map → filter → join → groupBy This plan helps optimize data processing by combining and pipelining operations. 8. What are transformations and actions in PySpark? Answer: • Transformations: Lazy operations that return a new RDD/DataFrame (e.g., filter(), map()) • Actions: Trigger execution and return results (e.g., collect(), count()) rdd = sc.parallelize([1, 2, 3]) rdd2 = rdd.map(lambda x: x * 2) # transformation result = rdd2.collect() # action Sud en G ora i","9. What is the lineage graph? Answer: Lineage graph shows the sequence of transformations leading to a result. It helps Spark recompute lost partitions in case of failure — key to fault tolerance. rdd.toDebugString() 10. What happens when you call an action on a transformation chain? Answer: Spark: 1. Builds a DAG 2. Optimizes execution plan 3. Submits the job 4. Executes the stages and tasks 5. Returns results \uD83D\uDCA1Example: data = sc.parallelize([1, 2, 3, 4]) data.map(lambda x: x*2).filter(lambda x: x > 4).collect() \uD83D\uDD39 PySpark – Section B: RDDs and DataFrames (Questions 11–20) 11. How do you create an RDD in PySpark? Answer: RDDs can be created from: • Existing collections • External data sources (files) rdd1 = spark.sparkContext.parallelize([1, 2, 3]) rdd2 = spark.sparkContext.textFile(\"data.txt\") Sud en G ora i","12. When should you use RDDs over DataFrames? Answer: Use RDDs when: • You need low-level transformation control • Your logic can't be expressed in SQL • You’re dealing with unstructured data Otherwise, DataFrames are more efficient due to Catalyst optimization and Tungsten execution. 13. What are the advantages of using DataFrames in PySpark? Answer: • Optimized execution via Catalyst Optimizer • Supports SQL-like syntax • Faster due to better memory usage and code generation • Easier integration with structured data formats 14. How do you convert RDD to DataFrame and vice versa? RDD ➡️ DataFrame: from pyspark.sql import Row rdd = spark.sparkContext.parallelize([Row(name=\"John\", age=30)]) df = spark.createDataFrame(rdd) DataFrame ➡️ RDD: df.rdd 15. How do you read a CSV/JSON file into a DataFrame? # CSV df_csv = spark.read.option(\"header\", True).csv(\"file.csv\") # JSON df_json = spark.read.json(\"file.json\") Sud en G ora i","16. Explain the difference between select() and withColumn(). • select() chooses columns: df.select(\"name\", \"age\") • withColumn() adds/modifies a column: df.withColumn(\"new_col\", df[\"age\"] + 1) 17. What is the use of cache() and persist() in PySpark? • cache(): Stores DataFrame in memory • persist(): Allows control over storage levels df.cache() df.persist(StorageLevel.DISK_ONLY) Useful when the DataFrame is reused across multiple actions. 18. How to filter rows in PySpark DataFrame? df.filter(df[\"age\"] > 25).show() # or df.where(\"age > 25\").show() 19. How to group data and apply aggregations in PySpark? df.groupBy(\"department\").agg( {\"salary\": \"avg\", \"bonus\": \"sum\"} ).show() More readable with functions: from pyspark.sql.functions import avg, sum df.groupBy(\"department\").agg( avg(\"salary\").alias(\"avg_salary\"), sum(\"bonus\").alias(\"total_bonus\") ).show() Sud en G ora i","20. What is the collect() method used for? Answer: Returns all elements of the DataFrame/RDD to the driver program as a list. result = df.collect() ⚠️ Avoid using on large datasets — it can cause Out of Memory (OOM) errors. \uD83D\uDD39 PySpark – Section C: Data Handling & Operations (Questions 21–30) 21. How do you handle missing or null values in PySpark? You can use: • dropna() – Drop rows with nulls • fillna() – Fill nulls with default values # Drop rows with any nulls df.na.drop() # Fill null values df.na.fill({\"age\": 0, \"name\": \"Unknown\"}) 22. How to drop duplicates in a DataFrame? df.dropDuplicates() # Drop duplicates based on specific columns df.dropDuplicates([\"name\", \"age\"]) 23. What is the difference between distinct() and dropDuplicates()? • distinct() removes duplicate rows entirely • dropDuplicates() can remove duplicates based on specific columns df.distinct() df.dropDuplicates([\"name\"]) Sud en G ora i","24. How to rename columns in a DataFrame? df = df.withColumnRenamed(\"old_col\", \"new_col\") To rename multiple columns: df = df.toDF(\"col1_new\", \"col2_new\", \"col3_new\") 25. How to perform joins in PySpark? df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\") df1.join(df2, [\"id\"], \"left\") Join types: \"inner\", \"left\", \"right\", \"outer\", \"left_semi\", \"left_anti\" 26. What is a broadcast join and when should you use it? A broadcast join sends a small DataFrame to all workers, avoiding shuffle. Use when one side of the join is very small. from pyspark.sql.functions import broadcast df1.join(broadcast(df2), \"id\") 27. How to change data types in a PySpark DataFrame? df = df.withColumn(\"age\", df[\"age\"].cast(\"int\")) Supported types: \"int\", \"string\", \"double\", \"boolean\", etc. 28. Explain how to use window functions. Window functions allow aggregations across partitions of data. from pyspark.sql.window import Window from pyspark.sql.functions import row_number windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\") df.withColumn(\"rank\", row_number().over(windowSpec)).show() Sud en G ora i","29. How do you repartition or coalesce a DataFrame? • repartition(n) reshuffles data into n partitions (more expensive) • coalesce(n) reduces to n partitions without full shuffle df.repartition(10) df.coalesce(2) Used to optimize performance or for efficient write operations. 30. How to write a DataFrame to Parquet/CSV/JSON format? # Parquet df.write.parquet(\"path/output\") # CSV df.write.option(\"header\", True).csv(\"path/output_csv\") # JSON df.write.json(\"path/output_json\") Add .mode(\"overwrite\") to overwrite existing files. \uD83D\uDD39 PySpark – Section D: Performance Tuning & Optimization (Expanded Answers) 31. How to improve the performance of your PySpark job? Answer: Performance optimization in PySpark is crucial when working with big data. Here are key tips: • ✅ Use DataFrames: They are optimized using the Catalyst Optimizer and run faster than RDDs. • ✅ Avoid collect() on large datasets: It brings data to the driver, which can cause memory overflow. • ✅ Cache or persist reused data: If a DataFrame is used multiple times, caching avoids recomputation. df.cache() Sud en G ora i","• ✅ Use broadcast joins for small lookup tables. from pyspark.sql.functions import broadcast df_large.join(broadcast(df_small), \"id\") • ✅ Repartition wisely: Avoid unnecessary shuffles. • ✅ Read and write in efficient formats like Parquet (columnar, supports predicate pushdown). 32. What is Tungsten in Spark? Answer: Tungsten is a major performance enhancement framework in Spark (since v1.4) that improves: • Memory management (manages memory manually outside the JVM heap) • CPU efficiency (uses cache-friendly binary format) • Code generation (uses runtime Java bytecode to reduce virtual function calls) \uD83D\uDCA1 It minimizes overhead of JVM objects and improves the speed of Spark operations. 33. What is Catalyst Optimizer? Answer: Catalyst is a powerful query optimization engine used by Spark SQL and DataFrames. It performs: 1. Analysis – Resolves references to columns and tables. 2. Logical optimization – Simplifies and combines expressions. 3. Physical planning – Generates multiple execution strategies. 4. Code generation – Uses Java bytecode for fast execution. \uD83D\uDD0D Example: df.explain(True) This shows you the entire logical and physical plan used to optimize your query. Sud en G ora i","34. What are the different types of joins in PySpark? Answer: PySpark supports multiple join types, just like SQL: Type Description inner Only matching rows from both datasets left All from left + matching from right right All from right + matching from left outer All from both, with nulls for missing matches left_semi Only left rows that have a match in right left_anti Only left rows that have no match in right \uD83D\uDD0D Example: df1.join(df2, \"id\", \"left_semi\") 35. What is shuffle and how can it be minimized? Answer: A shuffle is a costly operation where data is repartitioned across the network (e.g., in groupBy, join, distinct). ✅ Ways to minimize shuffle: • Use broadcast joins when one table is small • Avoid groupBy if reduceByKey or agg works • Use partitioning smartly before writing large datasets \uD83D\uDD0D Problem with shuffle: It can cause performance lag and out-of-memory errors. 36. What is partitioning in PySpark? Answer: Partitioning defines how Spark splits data across worker nodes. • Repartitioning increases the number of partitions (shuffle involved): df.repartition(10) • Coalesce reduces partitions without full shuffle: df.coalesce(2) Sud en G ora i","Use case: • Repartition before heavy parallel processing • Coalesce before writing to reduce small output files 37. Explain the difference between narrow and wide transformations. Type Description Example Narrow Each input partition maps to one output map, filter Wide Data shuffled across partitions (expensive) groupBy, join \uD83D\uDCA1 Wide transformations = network + disk IO → slower 38. How does PySpark handle memory management? Answer: Spark uses a unified memory model for: • Execution memory (shuffles, joins, aggregations) • Storage memory (cached data) You can control memory via: • spark.executor.memory (total executor memory) • spark.memory.fraction (fraction for execution/storage) • StorageLevel options for persisting: df.persist(StorageLevel.MEMORY_AND_DISK) 39. What is a skewed join? How do you handle it? Answer: In a skewed join, some keys have many more records than others, causing one task to do most of the work, while others are idle. \uD83D\uDEE0️ Solutions: • Salting: Add random prefix to the skewed key to spread data: from pyspark.sql.functions import concat, lit df_skewed = df.withColumn(\"skew_key\", concat(df[\"id\"], lit(\"_\", random_int))) • Use broadcast join if other side is small • Enable adaptive execution if on Spark 3.x+ Sud en G ora i","40. How can caching help improve performance? Answer: If a DataFrame is used multiple times, use cache() to: • Store it in memory • Avoid recomputation • Speed up multi-step pipelines df.cache() df.count() # First action triggers cache ⚠️ Monitor memory usage — cache only when reused multiple times and memory allows. \uD83D\uDD39 PySpark – Section E: Real-World Use Cases & Advanced Concepts (Questions 41–50) 41. How to handle schema evolution in PySpark? Answer: Schema evolution refers to handling changes in schema (new columns, data types, etc.) when reading/writing data. ✅ Tips to handle: • Use mergeSchema option when reading: df = spark.read.option(\"mergeSchema\", \"true\").parquet(\"path/\") • While writing, be consistent with schema or handle column addition manually. • With Delta Lake or Hudi (on top of Spark), schema evolution becomes easier. 42. How to load large datasets efficiently in PySpark? Answer: When working with large files (TBs), follow these best practices: 1. Read in parallel using multiple partitions: df = spark.read.option(\"multiLine\", \"false\").csv(\"path/\", inferSchema=True) 2. Use columnar formats like Parquet or ORC. 3. Filter early using .filter() or .where() to reduce data size. 4. Enable predicate pushdown (automatically works with Parquet). Sud en G ora i","5. Avoid loading all into driver (collect()). 43. How do you test PySpark code? Answer: For unit testing PySpark logic: • Use unittest or pytest in Python • Create a local SparkSession in test setup • Compare DataFrames using .collect() and assertEqual Example: def test_my_transformation(): spark = SparkSession.builder.master(\"local[*]\").getOrCreate() input_df = spark.createDataFrame([(1, \"a\")], [\"id\", \"val\"]) result_df = my_transformation(input_df) expected_df = spark.createDataFrame([(1, \"A\")], [\"id\", \"val\"]) assert result_df.collect() == expected_df.collect() 44. What are accumulators and broadcast variables? Answer: • Accumulators are write-only shared variables used for aggregation (like counters or sums). acc = spark.sparkContext.accumulator(0) rdd.foreach(lambda x: acc.add(1)) • Broadcast variables send read-only variables (like small lookup tables) to all executors to avoid shuffling. lookup = spark.sparkContext.broadcast({\"a\": 1, \"b\": 2}) 45. How can you integrate PySpark with Hive? Answer: Spark can connect with Hive using Hive support in SparkSession: spark = SparkSession.builder \\ .appName(\"HiveIntegration\") \\ .enableHiveSupport() \\ .getOrCreate() Sud en G ora i","# Query Hive table df = spark.sql(\"SELECT * FROM database.table\") ✅ Requirements: • Hive metastore must be accessible • Proper hive-site.xml configuration • Spark should have Hive support enabled 46. How to schedule and monitor PySpark jobs in production? Answer: ✅ Scheduling Tools: • Apache Airflow • Oozie • cron + shell script • Databricks Jobs UI • Apache Nifi ✅ Monitoring Tools: • Spark Web UI (usually on port 4040) • Spark History Server • Cloud-native monitoring: AWS CloudWatch, Azure Monitor, etc. 47. How to debug a failed Spark job? Answer: 1. Open Spark UI (localhost:4040) or History Server 2. Go to Stages and check failed stages/tasks 3. Look into the stderr logs of the failed executor 4. Check for: o OutOfMemoryError o File not found o Skewed data o Schema mismatch ✅ Tip: Use df.explain(True) to trace query plans before running. Sud en G ora i","48. How does checkpointing work in PySpark? Answer: Checkpointing helps persist intermediate RDDs to disk to avoid long lineage chains. sc.setCheckpointDir(\"/tmp/checkpoints\") rdd.checkpoint() ✅ Useful when: • Working with long chains of transformations • Fault-tolerance is critical ⚠️ You must trigger an action (like count()) for checkpointing to happen. 49. How to manage logs in Spark applications? Answer: • Spark uses Log4j for logging. • Set log level: spark.sparkContext.setLogLevel(\"ERROR\") • Logs are available: o In Spark UI → Executors tab o On YARN logs (yarn logs -applicationId) o On driver and executor nodes \uD83D\uDD0D Tip: External monitoring tools like ELK Stack, Datadog, or AWS CloudWatch can centralize logs. 50. Explain a real-world problem you solved using PySpark. Answer: Example (you can customize this): \"In a healthcare data project, we had to process daily medical records (10M+ rows/day) across various formats (CSV, Parquet, JSON). I built a PySpark ETL pipeline that read, cleaned, and aggregated the data for reporting and ML models. By using DataFrames, broadcast joins, and partitioning, we reduced the job time from 45 minutes to 8 minutes. We also added monitoring using Spark’s metrics and integrated with Airflow for daily scheduling.\" Sud en G ora i","\uD83D\uDD25 10 Complex PySpark Scenario-Based Questions & Answers 1. Scenario: Optimizing a Multi-Stage Join Pipeline Q: You have a pipeline that joins 5+ large tables (each 100M+ rows). It's slow, and shuffle spills to disk. How do you diagnose and optimize this? A: ✅ Step-by-step approach: • Profile each join cardinality and data size • Use Spark UI to identify shuffle stages & task skew • Apply broadcast join to the smallest dimension tables • Reorder joins: Join smallest tables first to reduce intermediate size • Use .explain(True) and df.queryExecution to understand physical plan \uD83D\uDD27 Example: # Broadcast the smallest df = fact_df \\ .join(broadcast(dim1), \"key1\") \\ .join(broadcast(dim2), \"key2\") \\ .join(dim3, \"key3\") # avoid broadcast if dim3 is large 2. Scenario: Implementing Slowly Changing Dimension (SCD Type 2) Q: You receive updated customer records daily. You need to implement SCD Type 2 using PySpark. How would you do this? A: ✅ Strategy: 1. Join current records with incoming data on business key 2. Identify: o Changed rows → mark old as expired & insert new o New rows → insert 3. Use a union of expired + new and write back \uD83D\uDD27 Example: from pyspark.sql.functions import current_timestamp, lit, when Sud en G ora i","# Step 1: Join existing and incoming df_joined = existing.join(incoming, \"customer_id\", \"outer\") # Step 2: Identify changes df_expired = df_joined.filter(\"existing.col1 != incoming.col1\") \\ .withColumn(\"is_current\", lit(False)) \\ .withColumn(\"end_date\", current_timestamp()) df_new = incoming.withColumn(\"is_current\", lit(True)) \\ .withColumn(\"start_date\", current_timestamp()) # Step 3: Final write df_final = df_expired.unionByName(df_new) 3. Scenario: Processing a Multi-Terabyte Dataset Without OOM Q: Your pipeline crashes due to Out-of-Memory errors while processing a multi-TB CSV dataset. What are your options? A: ✅ Solutions: • Switch to Parquet format → compressed & columnar • Use .repartition() to improve parallelism • Apply column pruning: only read required columns • Enable on-disk spill using persist(StorageLevel.DISK_ONLY) • Avoid collect(), toPandas() or UDFs on massive data df = spark.read.option(\"header\", True).parquet(\"s3://data/\") df = df.select(\"col1\", \"col2\").repartition(200) df.persist(StorageLevel.DISK_ONLY) Sud en G ora i","4. Scenario: Writing Daily Partitioned Data with Overwrite Logic Q: You need to overwrite only today’s partition (date=2025-07-02) while preserving the rest. How? A: ✅ Use dynamic partition overwrite: spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") df.write.partitionBy(\"date\").mode(\"overwrite\").parquet(\"s3://bucket/output/\") ⚠️ Without the config, Spark deletes all partitions when using overwrite. 5. Scenario: Real-Time Data Quality Alerts on Streaming Pipeline Q: You have a Kafka → Spark Structured Streaming → Delta Lake pipeline. How do you raise alerts when nulls or schema mismatches occur in real time? A: ✅ Build a side-streaming alert system: # Detect nulls or schema issues bad_data = df.filter(\"important_column IS NULL\") # Write to alert sink (e.g., Kafka, Slack, or logs) bad_data.writeStream \\ .format(\"console\") \\ .outputMode(\"append\") \\ .start() You can integrate with Kafka/SQS to push alerts or trigger downstream workflows via Airflow. 6. Scenario: Combining Batch and Streaming in the Same Pipeline Q: How would you design a unified architecture to handle both historical (batch) and real-time (streaming) data in PySpark? A: Use Lambda architecture or medallion design: • Batch Layer: ETL historical data into Bronze table (raw) • Streaming Layer: Read Kafka events into Bronze • Merge Layer: Combine both using structured streaming with Delta Lake merge Sud en G ora i","# Unified streaming + batch input df_batch = spark.read.parquet(\"s3://history/\") df_stream = spark.readStream.format(\"kafka\").load() # Merge batch + stream in Silver layer df_combined = df_batch.union(df_stream) 7. Scenario: Reduce Write Time for Millions of Records Q: Writing millions of records to disk takes 15 minutes. How to reduce it? A: ✅ Best practices: • Use Parquet instead of CSV • Use repartition() to write in parallel • Write with snappy compression • Avoid many tiny files → use coalesce() df.repartition(100).write.mode(\"overwrite\") \\ .option(\"compression\", \"snappy\") \\ .parquet(\"s3://fast-write/\") 8. Scenario: Skipping Corrupted Files During Read Q: Some files are corrupt in your input path, but you want to continue processing without failing the job. A: ✅ Use this config to skip corrupt files: spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\") df = spark.read.parquet(\"s3://data/\") Sud en G ora i","9. Scenario: Triggering Downstream Tasks After PySpark Completion Q: You want to notify an external system (e.g., Airflow or REST API) after the PySpark job finishes. What’s a clean way? A: ✅ Use a finally: block in Python or atexit, or call an API: import requests try: df.write.parquet(\"s3://output/\") finally: requests.post(\"https://your-system.com/api/job-complete\", json={\"status\": \"done\"}) For enterprise setups, you’d integrate with Airflow or a workflow orchestrator. 10. Scenario: Detecting and Mitigating Data Skew in GroupBy Operations Q: Your groupBy operation is extremely slow due to one group having millions of records. How to fix it? A: ✅ Strategy: • Identify skewed key distribution using: df.groupBy(\"group_col\").count().orderBy(\"count\", ascending=False).show() • Apply salting: from pyspark.sql.functions import rand, concat df = df.withColumn(\"salt\", (rand()*10).cast(\"int\")) df = df.withColumn(\"group_col_salted\", concat(\"group_col\", \"salt\")) • Group by salted key, then re-aggregate Sud en G ora i","\uD83D\uDD39 Spark SQL – Section A: Fundamentals & Syntax (Questions 1–10) 1. What is Spark SQL, and how is it different from Hive SQL? Answer: Spark SQL is a module of Apache Spark that enables SQL querying on structured data using DataFrames and SQL syntax. Feature Spark SQL Hive SQL Execution Engine Spark (in-memory) MapReduce/Tez (disk-based) Speed Very fast (in-memory) Slower (disk I/O heavy) API Support Python, Scala, Java, R, SQL Primarily SQL (via CLI or JDBC) Integration Supports Hive UDFs, metastore Tight integration with Hive Spark SQL can query Hive tables directly and can leverage Catalyst Optimizer for performance. 2. How do you create a temporary view from a DataFrame and query it using SQL? Answer: A temporary view is session-scoped and allows SQL-style querying. df = spark.read.csv(\"employees.csv\", header=True) df.createOrReplaceTempView(\"employees\") result = spark.sql(\"SELECT name, salary FROM employees WHERE salary > 50000\") result.show() createOrReplaceTempView() overwrites any existing view with the same name. 3. What is the difference between a temporary view and a global temporary view? Feature Temporary View (createOrReplaceTempView) Global Temp View (createGlobalTempView) Scope Current SparkSession only Shared across all sessions in app Access in SQL SELECT * FROM my_view SELECT * FROM global_temp.my_view Use global temporary views when the view must be accessed in multiple notebooks or jobs. Sud en G ora i","4. How do you execute SQL queries directly on a Parquet/CSV/JSON file in Spark SQL? Answer: Register the file as a temp view first: df = spark.read.option(\"header\", True).csv(\"data.csv\") df.createOrReplaceTempView(\"my_data\") spark.sql(\"SELECT * FROM my_data WHERE age > 25\").show() Spark doesn't need Hive metastore to run SQL on files. 5. How do you perform aggregations like GROUP BY, COUNT, and AVG in Spark SQL? SELECT department, COUNT(*) AS total_employees, AVG(salary) AS avg_salary FROM employees GROUP BY department These aggregations are optimized by Catalyst and run efficiently. 6. What is the spark.sql.shuffle.partitions setting used for? Answer: This setting controls the number of shuffle partitions Spark creates during operations like groupBy, join, etc. spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\") • Default is 200, which may be too high for small data and too low for big data. • Lower it to speed up small jobs and reduce output files. 7. How do you create a Spark SQL table from a DataFrame? df.write.saveAsTable(\"employee_table\") Requirements: • Spark must be configured with Hive support. • You can also use createOrReplaceTempView() for temporary use. 8. What’s the difference between saveAsTable and insertInto? Method Purpose Requirement saveAsTable Creates a new managed Hive-compatible table Can be used directly Sud en G ora i","Method Purpose Requirement insertInto Inserts data into existing table Table must exist with matching schema df.write.mode(\"overwrite\").saveAsTable(\"employee_new\") df.write.insertInto(\"employee_existing\") 9. How do you specify file format and partitioning while saving SQL tables? df.write \\ .format(\"parquet\") \\ .partitionBy(\"region\") \\ .saveAsTable(\"sales_data\") This creates a partitioned table stored in Parquet format, great for performance. 10. How to read Hive table data using Spark SQL? spark.sql(\"SELECT * FROM my_hive_table\").show() If Hive support is enabled, Spark can read: • Managed tables • External tables • Partitioned data Spark leverages Hive metastore but runs the query using in-memory execution, not MapReduce. \uD83D\uDD39 Spark SQL – Section B: Joins, Unions & Set Operations (Questions 11–20) Full explanations, syntax, and real-world use cases included 11. What are the different types of joins available in Spark SQL? Answer: Spark SQL supports standard SQL join types: Join Type Description INNER JOIN Returns only matching rows Sud en G ora i","Join Type Description LEFT JOIN All rows from left + matched from right RIGHT JOIN All rows from right + matched from left FULL OUTER All rows from both sides, with nulls for no match LEFT SEMI Returns rows from left that have match in right LEFT ANTI Returns rows from left without match in right Useful in data filtering, deduplication, and SCD logic. SELECT * FROM emp e INNER JOIN dept d ON e.dept_id = d.id 12. What is the difference between LEFT SEMI JOIN and INNER JOIN? Answer: Feature INNER JOIN LEFT SEMI JOIN Output columns Columns from both tables Columns from left table only Use case Combine data Filter based on existence in right LEFT SEMI JOIN is optimized for filtering with less data shuffle. SELECT * FROM employees e LEFT SEMI JOIN departments d ON e.dept_id = d.id 13. What is a LEFT ANTI JOIN, and when would you use it? Answer: Returns only the rows from the left table where no match exists in the right table. Use case: Identifying new or orphaned records SELECT * FROM employees e LEFT ANTI JOIN departments d ON e.dept_id = d.id Sud en G ora i","14. How to perform a self join in Spark SQL? Answer: Self join = joining a table with itself, typically on a condition like parent-child. SELECT a.emp_id, a.name AS employee, b.name AS manager FROM employees a JOIN employees b ON a.manager_id = b.emp_id 15. How do you combine data using UNION and UNION ALL in Spark SQL? Answer: • UNION: Removes duplicates (like DISTINCT) • UNION ALL: Keeps all rows, including duplicates SELECT name FROM india_sales UNION SELECT name FROM us_sales Schema must match in number and data types. 16. What’s the difference between JOIN and CROSS JOIN? Feature JOIN CROSS JOIN Matching Requires join condition All combinations (Cartesian product) Rows Returned Based on condition rowsA × rowsB Use CROSS JOIN with caution due to exponential row growth. SELECT * FROM products CROSS JOIN regions 17. How do you join more than two tables in Spark SQL? Answer: Chain multiple JOINs in a single SQL query: SELECT o.id, c.name, p.product_name FROM orders o JOIN customers c ON o.cust_id = c.id Sud en G ora i","JOIN products p ON o.prod_id = p.id Ensure correct join keys and order. 18. How do you handle null-safe equality in Spark SQL joins? Answer: Use the <=> operator to perform null-safe comparison (returns true if both sides are null). SELECT * FROM a JOIN b ON a.id <=> b.id Safer than = when columns might contain NULLs. 19. How do you optimize a join in Spark SQL for performance? Answer: Best practices: • Use broadcast joins for small right-hand datasets: from pyspark.sql.functions import broadcast df1.join(broadcast(df2), \"id\") • Reduce data shuffling: partition before joining large datasets • Filter early (pushdown predicates) • Use join hints in Spark 3+: SELECT /*+ BROADCAST(b) */ * FROM a JOIN b ON a.id = b.id 20. How to filter records that exist in one table but not the other? Answer: Use LEFT ANTI JOIN: SELECT * FROM source_table LEFT ANTI JOIN target_table ON source_table.id = target_table.id Efficient alternative to NOT EXISTS or NOT IN. Sud en G ora i","\uD83D\uDD39 Spark SQL – Section C: Aggregations, Window Functions & Advanced SQL (Questions 21–30) Detailed answers with real-world scenarios, SQL syntax, and optimization tips 21. How do you perform aggregations like SUM(), AVG(), MAX() in Spark SQL? Answer: Spark SQL supports all standard SQL aggregation functions: SELECT department, COUNT(*) AS total_employees, AVG(salary) AS avg_salary, MAX(salary) AS highest_salary FROM employees GROUP BY department Aggregations are executed in parallel and are optimized by Catalyst. 22. What is the purpose of the GROUP BY clause in Spark SQL? Answer: GROUP BY is used to group rows by one or more columns and apply aggregate functions on each group. Example: SELECT region, SUM(sales) FROM orders GROUP BY region Internally, Spark performs a shuffle to group records across partitions. 23. What are window functions in Spark SQL? Answer: Window functions compute values over a logical window (partition) of rows, without collapsing rows like GROUP BY. Use cases: • Ranking • Running totals • Lag/lead comparisons Sud en G ora i","SELECT name, department, RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_rank FROM employees 24. What’s the difference between RANK() and DENSE_RANK()? Function Behavior RANK() Skips ranks for ties (1,1,3,4...) DENSE_RANK() No skipped ranks (1,1,2,3...) Example: SELECT name, salary, RANK() OVER (ORDER BY salary DESC) AS rank, DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rank FROM employees 25. How to calculate a rolling sum (running total) in Spark SQL? Answer: Use SUM() with OVER clause and ROWS BETWEEN for rolling calculations. SELECT order_id, customer_id, order_amount, SUM(order_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total FROM orders Useful for financial reports, time-based analysis. 26. What does PARTITION BY do in a window function? Answer: It splits rows into groups for window function calculation. SELECT *, ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS row_num FROM employees Each department has its own ranking sequence. Sud en G ora i","27. What is the purpose of ROW_NUMBER() in Spark SQL? Answer: ROW_NUMBER() gives a unique sequential number to rows within a partition. Example: Get latest record per user: SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_time DESC) AS rn FROM events ) t WHERE rn = 1 Often used in deduplication, SCD Type 2, etc. 28. How do you use LAG() and LEAD() in Spark SQL? Answer: • LAG() returns a previous row’s value • LEAD() returns the next row’s value SELECT name, salary, LAG(salary, 1) OVER (ORDER BY hire_date) AS previous_salary FROM employees Used in change detection, trend comparison, etc. 29. How do you find duplicates using Spark SQL? Answer: You can use GROUP BY and HAVING: SELECT customer_id, COUNT(*) AS cnt FROM orders GROUP BY customer_id HAVING COUNT(*) > 1 Helps identify duplicate transactions, records needing deduplication. Sud en G ora i","30. How do you use conditional logic like CASE WHEN in Spark SQL? Answer: Use CASE WHEN to apply if-else logic in SQL: SELECT name, salary, CASE WHEN salary > 100000 THEN 'High' WHEN salary > 50000 THEN 'Medium' ELSE 'Low' END AS salary_band FROM employees Great for derived columns, categorization, and flagging logic. Spark SQL – Section D: Performance, Optimization & Query Planning (Questions 31–40) Full explanations, optimization tips, and interview-relevant syntax. 31. How does Spark SQL execute a query under the hood? Answer: Spark SQL query execution stages: 1. Parsing – SQL is parsed into an Abstract Syntax Tree (AST) 2. Logical Plan – AST is converted into a logical plan (no optimization yet) 3. Optimization (Catalyst) – Applies rule-based and cost-based optimizations 4. Physical Plan – Logical plan is translated into one or more physical plans 5. Execution – The best physical plan is chosen and run on the cluster Use: df.explain(True) To view logical and physical plans for debugging and optimization. 32. What is Catalyst Optimizer in Spark SQL? Answer: Catalyst is Spark’s query optimizer responsible for: • Predicate pushdown Sud en G ora i","• Constant folding • Column pruning • Join reordering • Subquery elimination It builds efficient query plans, enabling Spark to outperform traditional engines like Hive. 33. What is Tungsten in Spark SQL? Answer: Tungsten is Spark’s in-memory computation engine. It improves: • Memory management with off-heap storage • Code generation using whole-stage codegen • Binary processing (fewer object creations) Result: Reduced GC, faster execution, and less memory overhead. 34. How can you view and analyze a Spark SQL query plan? Answer: Use the following commands: df.explain(True) # detailed logical and physical plan df.queryExecution.logical # only logical plan df.queryExecution.optimizedPlan df.queryExecution.executedPlan Helps diagnose inefficient operations like Cartesian joins or unnecessary shuffles. 35. What is predicate pushdown, and why is it important? Answer: Predicate pushdown pushes filtering logic to the data source layer, reducing the amount of data read into Spark. Works best with formats like Parquet, ORC, Delta. SELECT * FROM orders WHERE order_date = '2025-01-01' With pushdown, only relevant file blocks are read. Sud en G ora i","36. How do you avoid full data shuffles in joins or aggregations? Answer: Strategies: • Broadcast joins for small lookup tables: df1.join(broadcast(df2), \"id\") • Partition before join: df1.repartition(\"key\").join(df2.repartition(\"key\"), \"key\") • Filter data early using .where() or .filter() • Avoid wide transformations when possible 37. How to tune Spark SQL performance for large datasets? Answer: Tuning checklist: • Adjust spark.sql.shuffle.partitions (default is 200) • Use columnar formats (Parquet, ORC) • Prefer DataFrames over RDDs • Use caching for reused data: df.cache() • Use broadcast joins for smaller tables • Enable adaptive query execution: spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\") 38. What is Adaptive Query Execution (AQE)? Answer: AQE dynamically adjusts execution plans at runtime based on actual statistics. Features: • Dynamically chooses join strategies • Dynamically coalesces shuffle partitions • Skew handling Enabled via: spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\") Sud en G ora i","39. What causes data skew in Spark SQL, and how do you handle it? Answer: Data skew happens when one key has disproportionately more data. Detection: SELECT key, COUNT(*) FROM table GROUP BY key ORDER BY 2 DESC Solutions: • Use salting (adding randomness to key) • Use broadcast join when possible • Enable AQE skew join handling 40. How can you optimize joins in Spark SQL with huge fact tables and multiple dimensions? Answer: Best Practices: • Use star schema and broadcast small dimension tables • Join small → medium → large tables in order • Avoid joins on non-partitioned, non-indexed columns • Use partition pruning Example: SELECT /*+ BROADCAST(d1) */ * FROM fact f JOIN dim1 d1 ON f.key1 = d1.key JOIN dim2 d2 ON f.key2 = d2.key You can also materialize intermediate steps as temp views to reduce recomputation. Sud en G ora i","\uD83D\uDD39 Spark SQL – Section E: Real-Time Analytics, Views, UDFs & Integration (Questions 41–50) Includes UDFs, streaming SQL, integrations with Hive, Delta, Kafka, and use cases 41. Can Spark SQL be used with Structured Streaming? Answer: Yes, Spark SQL can run continuous queries using Structured Streaming, especially on DataFrames with streaming sources like Kafka. df = spark.readStream \\ .format(\"kafka\") \\ .option(\"subscribe\", \"topic1\") \\ .load() df.createOrReplaceTempView(\"live_data\") spark.sql(\"SELECT value FROM live_data WHERE key = 'error'\") \\ .writeStream \\ .format(\"console\") \\ .start() Use views for live dashboards, fraud detection, real-time metrics. 42. How do you define and use a UDF (User Defined Function) in Spark SQL? Answer: UDFs are custom functions registered in Spark SQL to apply logic not supported natively. from pyspark.sql.functions import udf from pyspark.sql.types import StringType def mask_email(email): return email.split('@')[0][:2] + \"***@\" + email.split('@')[1] spark.udf.register(\"mask_email_udf\", mask_email, StringType()) spark.sql(\"SELECT mask_email_udf(email) FROM users\").show() Avoid UDFs if possible, as they break optimizations and slow down queries. Sud en G ora i","43. How do you create a view from Spark SQL and query it later? Answer: df.createOrReplaceTempView(\"sales\") spark.sql(\"\"\" CREATE OR REPLACE TEMP VIEW high_sales AS SELECT * FROM sales WHERE revenue > 10000 \"\"\") spark.sql(\"SELECT * FROM high_sales\").show() Use persistent views if you want them available across sessions. 44. How can you use Spark SQL with Delta Lake? Answer: Delta Lake brings ACID transactions and schema evolution. spark.sql(\"CREATE TABLE delta_sales USING DELTA LOCATION 's3://path/sales/'\") spark.sql(\"SELECT * FROM delta_sales WHERE year = 2025\") You can also do: • MERGE INTO for UPSERTs • DELETE, UPDATE, and schema evolution on-the-fly 45. How do you register and query Hive tables using Spark SQL? Answer: If Spark is built with Hive support and connected to Hive metastore: spark.sql(\"SELECT * FROM hive_db.hive_table\").show() Enables warehouse integration with Hive tables using Spark's in-memory engine. 46. Can Spark SQL read Kafka topics? How? Answer: Yes. Use readStream to consume from Kafka and apply SQL queries. df = spark.readStream \\ .format(\"kafka\") \\ .option(\"subscribe\", \"logs\") \\ Sud en G ora i",".option(\"kafka.bootstrap.servers\", \"broker:9092\") \\ .load() df.createOrReplaceTempView(\"kafka_data\") spark.sql(\"SELECT CAST(value AS STRING) FROM kafka_data\").writeStream.format(\"console\").start() Combine with watermarking and windowing for real-time analytics. 47. How do you perform schema evolution in Spark SQL using Delta Lake? Answer: Delta supports auto schema evolution via: df.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").save(\"/delta/events\") New columns are automatically added to the table schema. 48. How do you write SQL to upsert (insert or update) into a Delta table? Answer: Use MERGE INTO: MERGE INTO target_table t USING source_table s ON t.id = s.id WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT * Delta handles concurrency and file-level transactions. 49. How can Spark SQL integrate with BI tools like Power BI or Tableau? Answer: • Use Spark Thrift Server to expose Spark SQL via JDBC • Connect using JDBC URL: jdbc:spark://hostname:port Allows live queries over Spark SQL tables from Power BI, Tableau, and other tools. Sud en G ora i","50. What are some real-world use cases for Spark SQL in big data projects? Answer: • Ad-hoc querying on data lakes • ETL pipelines with complex transformations • Customer segmentation & behavior analysis • Real-time fraud detection using streaming SQL • Time-series aggregations for monitoring platforms • Data masking and compliance (GDPR, HIPAA) with UDFs • Sales & KPI dashboards for business teams Spark SQL is often the core engine in modern lakehouse or streaming architectures. Complex Real-World Spark SQL Scenarios – Questions & Answers 1. How do you implement Slowly Changing Dimension (SCD) Type 2 logic using Spark SQL and Delta Lake? Scenario: Maintain full history of changes in a customer table. MERGE INTO dim_customer AS target USING staged_customer AS source ON target.customer_id = source.customer_id AND target.is_current = true WHEN MATCHED AND target.hash_val <> source.hash_val THEN UPDATE SET is_current = false, end_date = current_date() WHEN NOT MATCHED THEN INSERT ( customer_id, name, address, hash_val, effective_date, end_date, is_current ) VALUES ( source.customer_id, source.name, source.address, source.hash_val, Sud en G ora i","current_date(), null, true ) Use hash_val to detect changes. Maintain effective_date, end_date, and is_current for history. 2. You're tasked with ingesting data from Kafka into a Delta Lake table using Spark SQL streaming. How do you ensure exactly-once semantics and partition-based optimization? df = spark.readStream \\ .format(\"kafka\") \\ .option(\"subscribe\", \"transactions\") \\ .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\ .option(\"startingOffsets\", \"earliest\") \\ .load() df_parsed = df.selectExpr(\"CAST(value AS STRING)\").withColumn(\"event_date\", to_date(col(\"timestamp\"))) df_parsed.writeStream \\ .format(\"delta\") \\ .outputMode(\"append\") \\ .partitionBy(\"event_date\", \"region\") \\ .option(\"checkpointLocation\", \"/delta/checkpoints/transactions\") \\ .start(\"/delta/bronze/transactions\") Ensures exactly-once via checkpointing. Partitioned by event_date and region for optimized querying. 3. You have a Delta table with evolving schema due to upstream source changes. How do you safely support schema evolution using Spark SQL without corrupting historical data? df.write \\ .format(\"delta\") \\ .option(\"mergeSchema\", \"true\") \\ .mode(\"append\") \\ .save(\"/delta/customer_events\") mergeSchema=true allows new fields to be added safely. Maintains backward compatibility across app versions. Sud en G ora i","4. You need to deduplicate a dataset with complex logic: keep the latest record based on event time but only if the status is completed. How do you achieve this using Spark SQL? WITH ranked_events AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY event_time DESC) AS rn FROM orders WHERE status = 'completed' ) SELECT * FROM ranked_events WHERE rn = 1 Efficient deduplication using ROW_NUMBER(). Only the latest 'completed' event is retained per order_id. 5. You notice a Spark SQL job performing a full shuffle join on two large tables. How do you refactor it using SQL-level broadcast hints or partitioning logic? SELECT /*+ BROADCAST(d) */ f.*, d.region FROM fact_orders f JOIN dim_region d ON f.region_id = d.region_id Use BROADCAST() hint for small dimension tables. Reduces shuffle overhead and speeds up joins. 6. You are asked to calculate a user retention metric by comparing users from day-0 and day-7. How do you write this using Spark SQL efficiently? WITH day_0_users AS ( SELECT user_id FROM user_logins WHERE login_date = '2025-01-01' ), day_7_users AS ( SELECT user_id FROM user_logins Sud en G ora i","WHERE login_date = '2025-01-08' ) SELECT COUNT(*) AS retained_users FROM day_0_users d0 JOIN day_7_users d7 ON d0.user_id = d7.user_id Great for cohort analysis and product stickiness tracking. 7. You have to compare two datasets for a data validation pipeline. How do you write a Spark SQL query that compares schema AND row-level content between two large tables? -- Rows in A not in B SELECT * FROM table_A EXCEPT SELECT * FROM table_B Use EXCEPT and INTERSECT for row-level diff. Combine with Python schema checks: df_a.columns == df_b.columns 8. You need to create a materialized view or cache layer using Spark SQL that refreshes daily but allows fast analytics. What strategy do you follow? daily_metrics = spark.sql(\"\"\" SELECT region, product_id, SUM(sales) AS total_sales FROM sales WHERE sale_date >= '2024-01-01' GROUP BY region, product_id \"\"\") daily_metrics.write \\ .format(\"delta\") \\ .mode(\"overwrite\") \\ .partitionBy(\"region\") \\ .save(\"/delta/cached/daily_sales\") Run daily ETL to refresh pre-aggregated insights. Faster queries for BI tools and dashboards. Sud en G ora i","9. A Spark SQL query is running slowly due to wide aggregations on skewed keys (e.g., city='New York'). How do you detect and resolve skew using SQL only? -- Detect skewed keys SELECT city, COUNT(*) AS cnt FROM orders GROUP BY city ORDER BY cnt DESC For salting: SELECT *, CONCAT(city, '-', CAST(FLOOR(RAND()*10) AS INT)) AS salted_key FROM orders Use salting to avoid skew. Or enable AQE skew join optimization: spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") 10. You need to implement audit logging for every change made via Spark SQL MERGE operations on a Delta table. How would you capture inserts, updates, and deletes? -- Enable Change Data Feed ALTER TABLE customer_data SET TBLPROPERTIES ( delta.enableChangeDataFeed = true ); -- Query change logs SELECT * FROM table_changes('customer_data', 5, 7) WHERE _change_type IN ('insert', 'update_postimage'); Enables row-level tracking for inserts, updates, deletes. Useful for auditing and CDC pipelines. Sud en G ora i"]}